# -*- coding: utf-8 -*-
"""RagWikiHow10%Mistral-7BInstruct.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12kLW-S4tdDS5bqEeN__c3lEvJ3ZYz7ln
"""

!pip install -q transformers sentence-transformers faiss-cpu datasets accelerate bitsandbytes

from transformers import pipeline
pipe = pipeline(
    "text-generation",
    model="mistralai/Mistral-7B-Instruct-v0.2",  # tested public model
    device_map="auto",
    torch_dtype="auto"

)

from datasets import load_dataset
ds = load_dataset("allenai/c4", "realnewslike", split="train[:1%]")
docs = [d["text"] for d in ds if d["text"]]
print(len(docs))

from sentence_transformers import SentenceTransformer
import numpy as np, faiss, gc

embedder = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = embedder.encode(docs, batch_size=64,
                              show_progress_bar=True, convert_to_numpy=True)

index = faiss.IndexFlatL2(embeddings.shape[1])
index.add(embeddings)
gc.collect()

def retrieve(query, k=10):
    q_emb = embedder.encode([query], convert_to_numpy=True)
    D, I = index.search(q_emb, k)
    return [docs[i] for i in I[0]]

def agentic_rag_stub(query):
    prompt = f"Answer in detail: {query}"
    out = pipe(prompt, max_new_tokens=200, temperature=0.7, top_p=0.9)
    return out[0]["generated_text"]

print(agentic_rag_stub("Tell me a quantization technique better than LoRA"))

def rag_pipeline(query, k=5):
    # Retrieve relevant documents
    retrieved_docs = retrieve(query, k)

    # Create a prompt with the retrieved documents
    context = "\n".join(retrieved_docs)
    prompt = f"Given the following context, answer the query in detail:\n\nContext:\n{context}\n\nQuery: {query}"

    # Generate the answer using the text generation pipeline
    out = pipe(prompt, max_new_tokens=500, temperature=0.7, top_p=0.9)
    return out[0]["generated_text"]

query = "is there an significant evnts before 10/11?"
answer = rag_pipeline(query)
print(answer)